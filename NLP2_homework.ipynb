{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3da55a25-b51f-41f5-83d2-3157ce70578f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import pandas as pd\n",
    "#import umap\n",
    "import nltk\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.stem import SnowballStemmer\n",
    "from pymorphy2 import MorphAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c8375c-d0fe-4ace-a003-cc32983176f1",
   "metadata": {},
   "source": [
    "## 1.NLP2_1 \n",
    "https://www.hackerrank.com/challenges/detect-the-email-addresses/problem?isFullScreen=true"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bba9c63d-fa85-4036-bd26-b8163c7003eb",
   "metadata": {},
   "source": [
    "import re\n",
    "Regex_Pattern = r'\\w+[\\w\\.]*\\w+@\\w+[\\w\\.]*\\w+'\n",
    "emails = []\n",
    "for i in range(int(input())):\n",
    "    string = str(input())\n",
    "    matches = re.findall(Regex_Pattern, string)\n",
    "    if matches:\n",
    "        for match in matches:\n",
    "            emails.append(match)\n",
    "emails = list(set(emails))\n",
    "emails.sort()\n",
    "print(';'.join(emails))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d5c49c-5e94-45a0-bb00-dbc890e81bb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dbd9e3ff-cde8-4a5a-ac80-1b06963a140d",
   "metadata": {},
   "source": [
    "## 2.NLP2_2\n",
    "https://www.hackerrank.com/challenges/detect-the-domain-name/problem?isFullScreen=true"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2a17700c-e8cc-466f-a91e-f2d767cc9c66",
   "metadata": {},
   "source": [
    "import re\n",
    "Regex_Pattern = r'(?<=://)[A-Za-z0-9-\\.]+\\.[A-Za-z]+'\n",
    "domains = []\n",
    "for i in range(int(input())):\n",
    "    string = str(input())\n",
    "    matches = re.findall(Regex_Pattern, string)\n",
    "    if matches:\n",
    "        for match in matches:                       \n",
    "            match = re.sub(r'ww[w2]\\.', '', match)\n",
    "            domains.append(match)\n",
    "domains = list(set(domains))\n",
    "domains.sort()\n",
    "print(';'.join(domains))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8207a85c-28fd-4834-9491-0383728d38d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245c88a3-e174-457e-bafa-54cb31fc7ac3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9237854a-6000-416e-816a-b8ee7d78c693",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1ff62c7-c85b-488a-9752-51a199aa258c",
   "metadata": {},
   "source": [
    "## 3. NLP2_3: \n",
    "### Реализовать stemming, lemmatization & BoW на следующем датасете: \n",
    "https://cloud.mail.ru/public/Z4L3/vB8GcgTtK (Russian Toxic-abuse comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c07a74d-5fa5-4c1a-af6a-58c379c7ad2b",
   "metadata": {},
   "source": [
    "### 1) Загрузка данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88704251-59c0-4f95-9d14-532b9cc92c8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Верблюдов-то за что? Дебилы, бл...\\n</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Хохлы, это отдушина затюканого россиянина, мол...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Собаке - собачья смерть\\n</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Страницу обнови, дебил. Это тоже не оскорблени...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>тебя не убедил 6-страничный пдф в том, что Скр...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14407</th>\n",
       "      <td>Вонючий совковый скот прибежал и ноет. А вот и...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14408</th>\n",
       "      <td>А кого любить? Гоблина тупорылого что-ли? Или ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14409</th>\n",
       "      <td>Посмотрел Утомленных солнцем 2. И оказалось, ч...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14410</th>\n",
       "      <td>КРЫМОТРЕД НАРУШАЕТ ПРАВИЛА РАЗДЕЛА Т.К В НЕМ Н...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14411</th>\n",
       "      <td>До сих пор пересматриваю его видео. Орамбо кст...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14412 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 comment  toxic\n",
       "0                   Верблюдов-то за что? Дебилы, бл...\\n    1.0\n",
       "1      Хохлы, это отдушина затюканого россиянина, мол...    1.0\n",
       "2                              Собаке - собачья смерть\\n    1.0\n",
       "3      Страницу обнови, дебил. Это тоже не оскорблени...    1.0\n",
       "4      тебя не убедил 6-страничный пдф в том, что Скр...    1.0\n",
       "...                                                  ...    ...\n",
       "14407  Вонючий совковый скот прибежал и ноет. А вот и...    1.0\n",
       "14408  А кого любить? Гоблина тупорылого что-ли? Или ...    1.0\n",
       "14409  Посмотрел Утомленных солнцем 2. И оказалось, ч...    0.0\n",
       "14410  КРЫМОТРЕД НАРУШАЕТ ПРАВИЛА РАЗДЕЛА Т.К В НЕМ Н...    1.0\n",
       "14411  До сих пор пересматриваю его видео. Орамбо кст...    0.0\n",
       "\n",
       "[14412 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"labeled.csv\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4506260-fcb3-4fc6-a60f-19d7f76ad9e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f392d651-5b9f-426e-9f47-2fedd8caafb9",
   "metadata": {},
   "source": [
    "### 2) Добавим все комментарии в список data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a773b5de-9168-4026-882b-74b6e8285a40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Верблюдов-то за что? Дебилы, бл...\\n',\n",
       " 'Хохлы, это отдушина затюканого россиянина, мол, вон, а у хохлов еще хуже. Если бы хохлов не было, кисель их бы придумал.\\n',\n",
       " 'Собаке - собачья смерть\\n',\n",
       " 'Страницу обнови, дебил. Это тоже не оскорбление, а доказанный факт - не-дебил про себя во множественном числе писать не будет. Или мы в тебя верим - это ты и твои воображаемые друзья?\\n',\n",
       " 'тебя не убедил 6-страничный пдф в том, что Скрипалей отравила Россия? Анализировать и думать пытаешься? Ватник что ли?)\\n',\n",
       " 'Для каких стан является эталоном современная система здравоохранения РФ? Для Зимбабве? Ты тупой? хохлы\\n',\n",
       " 'В шапке были ссылки на инфу по текущему фильму марвел. Эти ссылки были заменены на фразу Репортим брипидора, игнорируем его посты. Если этого недостаточно, чтобы понять, что модератор абсолютный неадекват, и его нужно лишить полномочий, тогда эта борда пробивает абсолютное дно по неадекватности.\\n',\n",
       " 'УПАД Т! ТАМ НЕЛЬЗЯ СТРОИТЬ! ТЕХНОЛОГИЙ НЕТ! РАЗВОРУЮТ КАК ВСЕГДА! УЖЕ ТРЕЩИНАМИ ПОШ Л! ТУПЫЕ КИТАЗЫ НЕ МОГУТ НИЧЕГО НОРМАЛЬНО СДЕЛАТЬ!\\n',\n",
       " 'Ебать тебя разносит, шизик.\\n',\n",
       " 'Обосрался, сиди обтекай\\n']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "for i in dataset.comment:\n",
    "    data.append(i)\n",
    "data[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1066f3-36f3-4858-aefb-f29a240946f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c67f5b0-fd0e-4007-ae16-c172f1ba3a4f",
   "metadata": {},
   "source": [
    "### 3) Токенизируем предложения (разобъём текст на более мелкие единицы)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "119a227b-ddc8-42e4-b3b2-288ddd207d71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['верблюдов', '-', 'то', 'за', 'что', '?', 'дебилы', ',', 'бл', '...']]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "data_tok = []\n",
    "for token in data:\n",
    "    current_token = tokenizer.tokenize(token)\n",
    "    row = []\n",
    "    for i in current_token:\n",
    "        row.append(i.lower())\n",
    "    data_tok.append(row)\n",
    "\n",
    "data_tok[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c32523-4a00-4fce-8e30-b8bd24e3c661",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20ef8944-6f3b-49cb-ad24-ade386ca346f",
   "metadata": {},
   "source": [
    "### 4) Стемминг (нахождение основы слова)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1f417d-8c37-4d32-b71e-758abedbd9b4",
   "metadata": {},
   "source": [
    "__4.1. Стемминг с помощью библиотеки PorterStemmer (для английских слов):__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e1487ccf-adad-4531-80bf-f23e029fa89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество унакальных слов:                 68638\n",
      "Количество уникальных слов после стемминга: 68602\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "\n",
    "all_words = []\n",
    "for i in data_tok:\n",
    "  for j in i:\n",
    "    all_words.append(j)\n",
    "\n",
    "all_stem = [ps.stem(word) for word in all_words]\n",
    "\n",
    "print(f\"Количество унакальных слов:                 {len(set(all_words))}\")\n",
    "print(f\"Количество уникальных слов после стемминга: {len(set(all_stem))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeda00ce-a18d-4dae-8b79-c1e6b8ea3693",
   "metadata": {},
   "source": [
    "Стемминг русских слов с помощью библиотеки PorterStemmer не сработал (но выборки немного уменьшилась, вероятно из-за наличия слов на английском)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f277e824-b63c-47a4-bb0e-eb5958927bad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f7ec44d-252f-49eb-8549-fae4272ef7a1",
   "metadata": {},
   "source": [
    "__4.2. Стемминг с помощью библиотеки SnowballStemmer(для русских слов):__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bdce9dc4-f8c5-45e9-9460-68083419de16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество унакальных слов:                 68638\n",
      "Количество уникальных слов после стемминга: 33642\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Создание объекта SnowballStemmer для русского языка\n",
    "snowball = SnowballStemmer(language=\"russian\")\n",
    "\n",
    "all_words = []\n",
    "for i in data_tok:\n",
    "  for j in i:\n",
    "    all_words.append(j)\n",
    "\n",
    "# Стемминг каждого слова\n",
    "all_stem = [snowball.stem(word) for word in all_words]\n",
    "\n",
    "print(f\"Количество унакальных слов:                 {len(set(all_words))}\")\n",
    "print(f\"Количество уникальных слов после стемминга: {len(set(all_stem))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d46a53-53fd-40a6-ac28-94a8e1d77bc3",
   "metadata": {},
   "source": [
    "Библиотека SnowballStemmer для стемминга сработала"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "44d871f5-53bd-40bf-93d9-f59a549f857c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество уникальных слов после стемминга: 33607\n"
     ]
    }
   ],
   "source": [
    "# Выполним стемминг оставшихся слов на английском\n",
    "all_stem_re = [ps.stem(word) for word in all_stem]\n",
    "\n",
    "print(f\"Количество уникальных слов после стемминга: {len(set(all_stem_re))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd94f9b-7e41-4b6b-944b-7dcc7b4cf794",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5858ab9-14bf-4168-81cd-11cbf0465c67",
   "metadata": {},
   "source": [
    "### 5) Лемматизация (начальная форма слова)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7056d91-f33a-4c79-82ba-b98e978242bf",
   "metadata": {},
   "source": [
    "__5.1. Лемматизация с помощью библиотеки WordNetLemmatizer (для английских слов):__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7f7cc6e3-b0ef-4c78-a94c-f95504782474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество унакальных слов:                    68638\n",
      "Количество уникальных слов после лемматизации: 68602\n"
     ]
    }
   ],
   "source": [
    "# Создание объекта WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "all_words = []\n",
    "for i in data_tok:\n",
    "  for j in i:\n",
    "    all_words.append(j)\n",
    "\n",
    "# Лемматизация каждого слова\n",
    "all_lem = [lemmatizer.lemmatize(word) for word in all_words]\n",
    "\n",
    "print(f\"Количество унакальных слов:                    {len(set(all_words))}\")\n",
    "print(f\"Количество уникальных слов после лемматизации: {len(set(all_lem))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b563cd-dd78-4bbe-95de-a2d57682f346",
   "metadata": {},
   "source": [
    "Лемматизация русских слов с помощью библиотеки WordNetLemmatizer так же не сработала (но выборки немного уменьшилась, вероятно из-за наличия слов на английском)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871101e6-f4e8-4355-830b-db1234fc2c14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c305d9e-c73c-49c1-8fd7-0c8e2363c328",
   "metadata": {},
   "source": [
    "__5.2. Лемматизация с помощью библиотеки MorphAnalyzer (для русских слов):__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "37288b12-986b-41da-9ebe-5160928bc855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество унакальных слов:                    68638\n",
      "Количество уникальных слов после лемматизации: 35494\n"
     ]
    }
   ],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "\n",
    "# Создание объекта MorphAnalyzer\n",
    "morph = MorphAnalyzer()\n",
    "\n",
    "all_words = []\n",
    "\n",
    "for i in data_tok:\n",
    "  for j in i:\n",
    "    all_words.append(j)\n",
    "\n",
    "# Лемматизация каждого слова\n",
    "all_lem = [morph.normal_forms(word)[0] for word in all_words]\n",
    "\n",
    "print(f\"Количество унакальных слов:                    {len(set(all_words))}\")\n",
    "print(f\"Количество уникальных слов после лемматизации: {len(set(all_lem))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206dccfa-7b9c-4893-9b74-04e21a509ef1",
   "metadata": {},
   "source": [
    "Библиотека pymorphy2 для лемматизации сработала"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b735b0b3-0f17-4eb9-87c5-55ecb3ca9a60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f29c52e5-3920-4629-856e-4133aab0c4c6",
   "metadata": {},
   "source": [
    "### 6) Стемминг с помощью библиотеки SnowballStemmer + лемматизация с помощью библиотеки MorphAnalyzer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ea3811aa-b509-4e72-ac27-e5e69ed3e7e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество унакальных слов:                                68638\n",
      "Количество уникальных слов после стемминга и лемматизации: 32027\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "snowball = SnowballStemmer(language=\"russian\")\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "morph = MorphAnalyzer()\n",
    "\n",
    "all_words = []\n",
    "for i in data_tok:\n",
    "  for j in i:\n",
    "    all_words.append(j)\n",
    "\n",
    "# стемминг и лемматизация русских слов\n",
    "all_stem_lem = [morph.normal_forms(snowball.stem(word))[0] for word in all_words]\n",
    "\n",
    "# Выполним стемминг и лемматизацию оставшихся слов на английском\n",
    "all_stem_lem_re = [lemmatizer.lemmatize(ps.stem(word)) for word in all_stem_lem]\n",
    "\n",
    "print(f\"Количество унакальных слов:                                {len(set(all_words))}\")\n",
    "print(f\"Количество уникальных слов после стемминга и лемматизации: {len(set(all_stem_lem_re))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6d5432-c5ef-4939-9a37-8ee028101c1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf3cbd0a-d1fb-4f7f-97f2-f21df8d8807a",
   "metadata": {},
   "source": [
    "__Поменяем местами SnowballStemmer и pymorphy2:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9bf37dfe-dbd5-48c2-8420-08c7de742634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество унакальных слов:                                68638\n",
      "Количество уникальных слов после лемматизации и стемминга: 32027\n"
     ]
    }
   ],
   "source": [
    "snowball = SnowballStemmer(language=\"russian\")\n",
    "morph = MorphAnalyzer()\n",
    "\n",
    "all_words = []\n",
    "for i in data_tok:\n",
    "  for j in i:\n",
    "    all_words.append(j)\n",
    "\n",
    "# лемматизация и стемминг русских слов\n",
    "all_lem_stem = [snowball.stem(morph.normal_forms(word)[0]) for word in all_words]\n",
    "\n",
    "# Выполним лемматизацию и стемминг оставшихся слов на английском\n",
    "all_lem_stem_re = [ps.stem((lemmatizer.lemmatize(word))) for word in all_lem_stem]\n",
    "\n",
    "print(f\"Количество унакальных слов:                                {len(set(all_words))}\")\n",
    "print(f\"Количество уникальных слов после лемматизации и стемминга: {len(set(all_stem_lem_re))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db12724-3751-4a6f-ac2b-efda29b597e7",
   "metadata": {},
   "source": [
    "__Результат такой же => порядок выполнения не играет роли__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c3db59-23e0-4d3a-b853-7e5aedfff56a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b37e0854-f606-49d0-85a6-4c036cd5dacf",
   "metadata": {},
   "source": [
    "### 7) Лемматизации + стемминг + BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "afcc9127-df51-4c8e-9774-17ff6a09d069",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/natalianekrasova/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Множество стоп-слов\n",
      "{'вдруг', 'него', 'лучше', 'тем', 'зачем', 'такой', 'когда', 'между', 'к', 'от', 'куда', 'над', 'опять', 'нас', 'тогда', 'мне', 'два', 'другой', 'потом', 'как', 'меня', 'вас', 'можно', 'теперь', 'мы', 'хоть', 'на', 'сейчас', 'три', 'тот', 'один', 'всегда', 'все', 'во', 'этого', 'никогда', 'этом', 'нее', 'ну', 'же', 'чтоб', 'тебя', 'только', 'разве', 'почти', 'них', 'даже', 'ним', 'о', 'быть', 'за', 'если', 'эту', 'уже', 'конечно', 'была', 'более', 'нет', 'этот', 'хорошо', 'уж', 'она', 'из', 'под', 'всю', 'было', 'иногда', 'через', 'эти', 'ей', 'с', 'при', 'много', 'ж', 'но', 'был', 'совсем', 'себя', 'чего', 'всего', 'мой', 'им', 'кто', 'всех', 'вы', 'надо', 'со', 'чуть', 'об', 'том', 'про', 'какая', 'для', 'а', 'ни', 'там', 'потому', 'вот', 'без', 'ее', 'вам', 'ему', 'раз', 'они', 'ли', 'ней', 'перед', 'больше', 'чем', 'нибудь', 'были', 'бы', 'где', 'ты', 'ничего', 'есть', 'до', 'его', 'будет', 'свою', 'так', 'еще', 'у', 'того', 'этой', 'в', 'себе', 'или', 'то', 'что', 'по', 'какой', 'может', 'нельзя', 'наконец', 'их', 'я', 'не', 'после', 'будто', 'здесь', 'чтобы', 'впрочем', 'моя', 'и', 'да', 'тут', 'тоже', 'ведь', 'он', 'сам'}\n",
      "Количество уникальных слов в датасете:                              30881\n",
      "Количество уникальных слов в датасете без стоп-слов и спецсимволов: 30805\n",
      "Количество векторов:                                                14412\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def vectorize(tokens):\n",
    "    \n",
    "    ''' Функция vectorize(tokens) принимает в качестве аргумента список tokens (токенов) \n",
    "    и создает вектор, представляющий количество вхождений каждого слова из заранее \n",
    "    отфильтрованного словаря filtered_vocab в переданном списке токенов'''\n",
    "    \n",
    "    vector=[]\n",
    "    for w in filtered_vocab:\n",
    "        vector.append(tokens.count(w))\n",
    "    return vector\n",
    "\n",
    "def unique(sequence):\n",
    "    \n",
    "    '''Функция unique(sequence) принимает последовательность sequence и возвращает \n",
    "    список уникальных элементов из этой последовательности, сохраняя порядок, в котором \n",
    "    элементы встречаются в исходной последовательности.'''\n",
    "    \n",
    "    seen = set()\n",
    "    return [x for x in sequence if not (x in seen or seen.add(x))]\n",
    "\n",
    "data_tok_sample = data_tok\n",
    "\n",
    "#создадим список стоп-слов:\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('russian'))\n",
    "print(\"Множество стоп-слов\")\n",
    "print(stop_words)\n",
    "\n",
    "#список специальых символов:\n",
    "special_char=[\",\",\":\",\" \",\";\",\".\",\"?\"]\n",
    "\n",
    "#для русских слов используем SnowballStemmer и MorphAnalyzer, для английских PorterStemmer и WordNetLemmatizer\n",
    "snowball = SnowballStemmer(language=\"russian\")\n",
    "morph = MorphAnalyzer()\n",
    "ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "data_tok_lem_stem = []\n",
    "for i in data_tok_sample:\n",
    "    string = []\n",
    "    for j in i:\n",
    "        # лемматизация и стемминг русских слов \n",
    "        word_r = snowball.stem(morph.normal_forms(j)[0])\n",
    "        # лемматизация и стемминг английских слов\n",
    "        word_re = ps.stem((lemmatizer.lemmatize(word_r)))\n",
    "        string.append(word_re)\n",
    "    data_tok_lem_stem.append(string)\n",
    "      \n",
    "#создадим список из data_tok_lem_stem:\n",
    "all_lem_stem_words = []\n",
    "for i in data_tok_lem_stem:\n",
    "  for j in i:\n",
    "    all_lem_stem_words.append(j)\n",
    "\n",
    "#найдём уникальные слова в датасете:\n",
    "vocab=unique(all_lem_stem_words)\n",
    "\n",
    "print(f\"Количество уникальных слов в датасете:                              {len(vocab)}\")\n",
    "\n",
    "#отфильтруем этот датасет:\n",
    "filtered_vocab=[]\n",
    "for w in vocab: \n",
    "    if w not in stop_words and w not in special_char: \n",
    "        filtered_vocab.append(w)\n",
    "print(f\"Количество уникальных слов в датасете без стоп-слов и спецсимволов: {len(filtered_vocab)}\")\n",
    "\n",
    "# векторизация датасета:\n",
    "data_tok_lem_stem_vectors = []\n",
    "for i in data_tok_lem_stem:\n",
    "    data_tok_lem_stem_vectors.append(vectorize(i))\n",
    "print(f\"Количество векторов:                                                {len(data_tok_lem_stem_vectors)}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4653eb04-8bf8-4d0a-ad38-caa227492ac6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "741b2b2b-4b51-4370-a93e-2fbb5419d7e9",
   "metadata": {},
   "source": [
    " ### 8) Поиск самых схожих объектов (по максимальной косинусной мере) - по первым 100  элементам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3eb4c8a0-07eb-430c-9672-1f0565ac6461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наиболее схожие строки:\n",
      "['возьмём', 'как', 'пример', 'россию', ',', 'западноевропейские', 'страны', 'и', 'сша', '.', 'идёт', 'метисация', ',', 'сознательная', 'политика', 'замещения', 'белого', 'населения', 'на', 'пришлое', 'черно', '-', 'коричневое', '.', 'идёт', 'создание', 'новой', 'расы', 'метисов', ',', 'исламизация', 'и', 'почернение', '.', 'в', 'крупных', 'городах', 'половина', 'населения', '-', 'выходцы', 'из', 'ебеней', 'мексики', ',', 'африки', ',', 'ближнего', 'востока', ',', 'а', 'в', 'случае', 'с', 'россией', '-', 'кавказа', 'и', 'средней', 'азии', '.', 'этнические', 'ниггеро', '-', 'арабские', 'гетто', 'верят', 'на', 'хую', 'законы', 'как', 'хотят', ',', 'чудовищная', 'по', 'масштабам', 'этническая', 'преступность', '.', 'говорить', 'о', 'миграции', 'и', 'тем', 'более', 'затрагивать', 'тему', 'замещения', 'коренного', 'населения', 'властями', 'нельзя', ',', 'иначе', 'бутылка', '.', 'свобода', 'слова', 'тут', 'не', 'для', 'вас', ',', 'молодой', 'человек', '.', 'при', 'этом', 'говорить', 'о', 'том', ',', 'что', 'белые', 'должны', 'вымереть', ',', 'и', 'это', 'нормально', '-', 'можно', '.', 'белые', 'официально', 'вымирают', 'ведётся', 'пропаганда', 'так', 'или', 'иначе', 'направленная', 'на', 'снижение', 'рождаемости', 'белого', 'населения', '.', 'феминизм', ',', 'лгбт', ',', 'чайлдфри', '.', 'каждая', 'женщина', 'в', 'швеции', '-', 'леволиберальная', 'феминистка', ',', 'это', 'страна', 'победившего', 'феминизма', '.', 'что', 'сегодня', 'там', 'происходит', '-', 'страшно', 'делается', '.', 'пропагандируются', 'смешанные', 'браки', ',', 'межрасовые', 'браки', ',', 'пропагандируется', 'превосходство', 'детей', '-', 'метисов', '.', 'идёт', 'демонизация', 'белых', 'и', 'пропаганда', 'превосходства', 'чёрных', 'и', 'смуглых', 'мужчин', ',', 'форс', 'отношений', 'белая', 'женщина', 'смуглый', 'чёрный', 'мужчина', '-', 'мигрант', '.', 'как', 'результат', '-', 'всё', 'больше', 'чернильниц', ',', 'всё', 'больше', 'смешанных', 'браков', ',', 'всё', 'больше', 'небелых', 'метисов', '.', 'белые', 'женщины', 'просто', 'не', 'хотят', 'контактировать', 'с', 'мужчинами', 'своей', 'нации', 'и', 'расы', ',', 'наделяя', 'их', 'самыми', 'плохими', 'качествами', 'и', 'обожествляя', 'черных', '.', 'при', 'этом', 'большинство', 'белых', 'не', 'считает', 'завоз', 'чурок', 'чем', '-', 'то', 'плохим', ',', 'наоборот', ',', 'относятся', 'к', 'ним', 'толерантно', '.', 'проводится', 'политика', 'насаждения', 'толерантности', ',', 'мультикультурализма', ',', 'политкорректности', 'и', 'космополитизма', '.', 'набирающее', 'популярность', 'даже', 'в', 'россии', 'sjw', '-', 'это', 'вообще', 'отдельная', 'тема', 'для', 'обсуждения', '.', 'всё', 'вышеперечисленное', 'относится', 'к', 'сильнейшим', 'когда', '-', 'то', 'странам', ',', 'бывшим', 'империям', ',', 'нагибающим', 'слабых', '.', 'сегодня', 'происходит', 'так', ',', 'что', 'бывшие', 'империи', 'в', 'прямом', 'смысле', 'деградируют', ',', 'вырождаются', 'и', 'вымирают', ',', 'а', 'место', 'сильнейших', 'когда', '-', 'то', ',', 'господствующих', 'народов', ',', 'занимают', 'те', ',', 'кого', 'когда', '-', 'то', 'колонизировали', '.', 'во', 'франции', 'к', '2080', 'уже', 'будут', 'доминировать', 'негры', 'и', 'арабы', ',', 'в', 'россии', '-', 'кавказцы', 'и', 'выходцы', 'из', 'средней', 'азии', ',', 'в', 'великобритании', '-', 'индийцы', ',', 'негры', ',', 'арабы', ',', 'пакистанцы', ',', 'etc', '.', 'а', 'в', 'маленьких', ',', 'нейтральных', 'странах', ',', 'вроде', 'словении', 'или', 'беларуси', ',', 'литвы', 'или', 'чехии', ',', 'румынии', 'или', 'эстонии', '-', 'всё', 'пучком', '.', 'им', 'вымирание', 'не', 'грозит', ',', 'они', 'остаются', 'и', 'будут', 'оставаться', 'белыми', '.', 'более', 'того', ',', 'у', 'них', 'ведётся', 'политика', ',', 'направленная', 'на', 'сохранение', 'традиционных', 'ценностей', 'и', 'культуры', 'коренного', 'населения', '.', 'они', 'сказали', 'беженцам', 'нет', '.', 'в', 'польшу', ',', 'например', ',', 'русскому', 'или', 'украинцу', 'гораздо', 'легче', 'переехать', 'и', 'остаться', ',', 'чем', 'арабу', 'или', 'африканцу', '.', 'в', 'германии', 'ситуация', 'противоположная', ',', 'белых', 'там', 'не', 'ждут', '.', 'польша', ',', 'чехия', ',', 'словакия', ',', 'венгрия', ',', 'словения', ',', 'хорватия', ',', 'сербия', ',', 'биг', ',', 'черногория', ',', 'македония', ',', 'греция', ',', 'болгария', ',', 'румыния', ',', 'молдова', ',', 'украина', ',', 'беларусь', ',', 'литва', ',', 'латвия', ',', 'эстония', '-', 'вот', 'европа', 'будущего', '.', 'скандинавия', ',', 'южная', ',', 'западная', 'европа', ',', 'а', 'также', 'россия', '-', 'лишатся', 'коренного', 'населения', 'и', 'своей', 'культуры', '.']\n",
      "['тупой', '-', 'тупой', '-', 'тупая', ',', 'ти', '-', 'ля', '-', 'ля', '-', 'лю', '-', 'лю', '!']\n",
      "['взят', 'как', 'пример', 'росс', ',', 'западноевропейск', 'стран', 'и', 'сша', '.', 'идт', 'метисац', ',', 'сознательн', 'политик', 'замещен', 'бел', 'населен', 'на', 'пришл', 'черн', '-', 'коричнев', '.', 'идт', 'создан', 'нов', 'рас', 'метис', ',', 'исламизац', 'и', 'почернен', '.', 'в', 'крупн', 'город', 'половин', 'населен', '-', 'выходец', 'из', 'ебен', 'мексик', ',', 'африк', ',', 'ближн', 'восток', ',', 'а', 'в', 'случа', 'с', 'росс', '-', 'кавказ', 'и', 'средн', 'аз', '.', 'этническ', 'ниггер', '-', 'арабск', 'гетт', 'вер', 'на', 'ху', 'закон', 'как', 'хотет', ',', 'чудовищн', 'по', 'масштаб', 'этническ', 'преступн', '.', 'говор', 'о', 'миграц', 'и', 'тем', 'бол', 'затрагива', 'тем', 'замещен', 'корен', 'населен', 'власт', 'нельз', ',', 'инач', 'бутылк', '.', 'свобод', 'слов', 'тут', 'не', 'для', 'вы', ',', 'молод', 'человек', '.', 'при', 'эт', 'говор', 'о', 'тот', ',', 'что', 'бел', 'должн', 'вымерет', ',', 'и', 'эт', 'нормальн', '-', 'можн', '.', 'бел', 'официальн', 'вымира', 'вест', 'пропаганд', 'так', 'ил', 'инач', 'направ', 'на', 'снижен', 'рождаем', 'бел', 'населен', '.', 'феминизм', ',', 'лгбт', ',', 'чайлдфр', '.', 'кажд', 'женщин', 'в', 'швец', '-', 'леволиберальн', 'феминистк', ',', 'эт', 'стран', 'побед', 'феминизм', '.', 'что', 'сегодн', 'там', 'происход', '-', 'страшн', 'дела', '.', 'пропагандирова', 'смеша', 'брак', ',', 'межрасов', 'брак', ',', 'пропагандирова', 'превосходств', 'ребенок', '-', 'метис', '.', 'идт', 'демонизац', 'бел', 'и', 'пропаганд', 'превосходств', 'черн', 'и', 'смугл', 'мужчин', ',', 'форс', 'отношен', 'бел', 'женщин', 'смугл', 'черн', 'мужчин', '-', 'мигрант', '.', 'как', 'результат', '-', 'ве', 'больш', 'чернильниц', ',', 'ве', 'больш', 'смеша', 'брак', ',', 'ве', 'больш', 'небел', 'метис', '.', 'бел', 'женщин', 'прост', 'не', 'хотет', 'контактирова', 'с', 'мужчин', 'сво', 'нац', 'и', 'рас', ',', 'наделя', 'он', 'сам', 'плох', 'качеств', 'и', 'обожествля', 'черн', '.', 'при', 'эт', 'большинств', 'бел', 'не', 'счита', 'завоз', 'чурк', 'чем', '-', 'то', 'плох', ',', 'наоборот', ',', 'относ', 'к', 'он', 'толерантн', '.', 'провод', 'политик', 'насажден', 'толерантн', ',', 'мультикультурализм', ',', 'политкорректн', 'и', 'космополитизм', '.', 'набира', 'популярн', 'даж', 'в', 'росс', 'sjw', '-', 'эт', 'вообщ', 'отдельн', 'тем', 'для', 'обсужден', '.', 'ве', 'вышеперечислен', 'относ', 'к', 'сильн', 'когд', '-', 'то', 'стран', ',', 'бывш', 'импер', ',', 'нагиба', 'слаб', '.', 'сегодн', 'происход', 'так', ',', 'что', 'бывш', 'импер', 'в', 'прям', 'смысл', 'деградирова', ',', 'вырожда', 'и', 'вымира', ',', 'а', 'мест', 'сильн', 'когд', '-', 'то', ',', 'господств', 'народ', ',', 'занима', 'тот', ',', 'кто', 'когд', '-', 'то', 'колонизирова', '.', 'в', 'франц', 'к', '2080', 'уж', 'быт', 'доминирова', 'негр', 'и', 'араб', ',', 'в', 'росс', '-', 'кавказец', 'и', 'выходец', 'из', 'средн', 'аз', ',', 'в', 'великобритан', '-', 'индиец', ',', 'негр', ',', 'араб', ',', 'пакистанец', ',', 'etc', '.', 'а', 'в', 'маленьк', ',', 'нейтральн', 'стран', ',', 'врод', 'словен', 'ил', 'белар', ',', 'литв', 'ил', 'чех', ',', 'румын', 'ил', 'эстон', '-', 'ве', 'пучок', '.', 'им', 'вымиран', 'не', 'гроз', ',', 'он', 'остава', 'и', 'быт', 'остава', 'бел', '.', 'бол', 'тот', ',', 'у', 'он', 'вест', 'политик', ',', 'направ', 'на', 'сохранен', 'традицион', 'ценност', 'и', 'культур', 'корен', 'населен', '.', 'он', 'сказа', 'беженец', 'нет', '.', 'в', 'польш', ',', 'например', ',', 'русск', 'ил', 'украинец', 'горазд', 'легк', 'перееха', 'и', 'оста', ',', 'чем', 'араб', 'ил', 'африканец', '.', 'в', 'герман', 'ситуац', 'противоположн', ',', 'бел', 'там', 'не', 'ждат', '.', 'польш', ',', 'чех', ',', 'словак', ',', 'венгр', ',', 'словен', ',', 'хорват', ',', 'серб', ',', 'биг', ',', 'черногор', ',', 'македон', ',', 'грец', ',', 'болгар', ',', 'румын', ',', 'молдов', ',', 'украин', ',', 'белар', ',', 'литв', ',', 'латв', ',', 'эстон', '-', 'вот', 'европ', 'будущ', '.', 'скандинав', ',', 'южн', ',', 'западн', 'европ', ',', 'а', 'такж', 'росс', '-', 'лиш', 'корен', 'населен', 'и', 'сво', 'культур', '.']\n",
      "['туп', '-', 'туп', '-', 'туп', ',', 'ти', '-', 'ля', '-', 'ля', '-', 'лю', '-', 'лю', '!']\n",
      "Косинусная мера между строками: 0.4928224924937734\n"
     ]
    }
   ],
   "source": [
    "#Создается пустой список matrix, в который будут добавляться рассчитанные значения\n",
    "matrix = []\n",
    "\n",
    "for a in data_tok_lem_stem_vectors[:100]:      #Для каждого элемента a из первых 100 элементов data_tok_lem_stem_vectors\n",
    "    string = []                                #создается временный список string.\n",
    "    for b in data_tok_lem_stem_vectors[:100]:  #Для каждого элемента b из первых 100 элементов data_tok_lem_stem_vectors, \n",
    "        string.append(dot(a, b)/(norm(a)*norm(b))) #рассчитывается косинусное сходство между a и b и результат добавляется в список string\n",
    "    matrix.append(string)                      #затем список string с косинусными мерами для a и всех b добавляется в общий список matrix.\n",
    "df = pd.DataFrame(matrix)                      #matrix преобразуется в DataFrame с помощью библиотеки Pandas. \n",
    "\n",
    "\n",
    "for i in range(df.shape[0]):                   #далее происходит итерация по диагонали DataFrame, \n",
    "    df.iat[i,i] = 0                            #устанавливаются значения в нуль.\n",
    "str = df.max().idxmax()                        #находится строка с максимальным значением, \n",
    "col = df.loc[str,:].idxmax()                   #затем в этой строке находится столбец с максимальным значением.\n",
    "print(\"Наиболее схожие строки:\")\n",
    "print(data_tok_sample[str])                    #Выводятся пары строк и их оригинальный текст из data_tok_sample и их \n",
    "print(data_tok_sample[col])       \n",
    "print(data_tok_lem_stem[str])                  #Лемматизированный текст из data_tok_lem_stem,\n",
    "print(data_tok_lem_stem[col])                  #под которыми находятся наиболее схожие друг с другом строки.\n",
    "print(f\"Косинусная мера между строками: {df.max().max()}\") #Выводится значение косинусной меры между этими строками.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855a3a7d-eaf7-41d2-8955-6c6cf683792b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbbd4d6-5f2c-4652-9ad4-86772e1983c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9301c6-fc8b-4acf-a176-a782b8c97e49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20218608-56d1-4f1a-bb90-b89e4d1cb140",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86eb4b3e-8dd1-4bca-ab96-d68826bd8e46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc06959-5153-4d4c-b85f-5874d26af9b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d111c90c-3164-4623-a8ad-756ad0c3713e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb88fa6-627a-48f2-9613-f1ca909a82d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbfbad5-17f0-4a4c-9b9b-4b13e0e7b2c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4e9a59-4224-4575-b832-d7e9f2b2ec19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66f370d-46cd-49ae-b99c-6cdcd0f55877",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148e05ec-6a74-4411-8025-7acfdce75507",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f9d133-2897-4684-91b6-f62ea20326ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
